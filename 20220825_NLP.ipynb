{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a9e7c9",
   "metadata": {},
   "source": [
    "# 1.How to tokenize a given text?\n",
    "text= \"Last week, the University of Cambridge shared its own \n",
    "research that shows if everyone wears a mask outside home,dreaded \n",
    "‘second wave’ of the pandemic can be avoided.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383bcd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0d40a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2d7c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b379a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last\n",
      "week\n",
      ",\n",
      "the\n",
      "University\n",
      "of\n",
      "Cambridge\n",
      "shared\n",
      "its\n",
      "own\n",
      "research\n",
      "that\n",
      "shows\n",
      "if\n",
      "everyone\n",
      "wears\n",
      "a\n",
      "mask\n",
      "outside\n",
      "home\n",
      ",\n",
      "dreaded\n",
      "‘\n",
      "second\n",
      "wave\n",
      "’\n",
      "of\n",
      "the\n",
      "pandemic\n",
      "can\n",
      "be\n",
      "avoided\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Tokeniation with nltk\n",
    "\n",
    "text= \"\"\"Last week, the University of Cambridge shared its own \n",
    "research that shows if everyone wears a mask outside home,dreaded \n",
    "‘second wave’ of the pandemic can be avoided.\"\"\"\n",
    "\n",
    "tokens=nltk.word_tokenize(text)\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f982ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b82eb5ad",
   "metadata": {},
   "source": [
    "# 2. How to tokenize a text using the `transformers` package ?\n",
    "text=\"I love spring season. I go hiking with my friends\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73bc0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f464ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2293, 3500, 2161, 1012, 1045, 2175, 13039, 2007, 2026, 2814, 102]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] i love spring season. i go hiking with my friends [SEP]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import tokenizer from transfromers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "text=\"I love spring season. I go hiking with my friends\"\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Encoding with the tokenizer\n",
    "inputs=tokenizer.encode(text)\n",
    "print(inputs)\n",
    "tokenizer.decode(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f956b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "733a8d87",
   "metadata": {},
   "source": [
    "# 3. How to remove stop words in a text ?\n",
    " Remove all the stop words ( ‘a’ , ‘the’, ‘was’…) from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332cdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64f941a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'outbreak coronavirus disease 2019 ( COVID-19 ) created global health crisis deep impact way perceive world everyday lives . Not rate contagion patterns transmission threatens sense agency , safety measures put place contain spread virus also require social distancing refraining inherently human , find solace company others . Within context physical threat , social physical distancing , well public alarm , ( ) role different mass media channels lives individual , social societal levels ? Mass media long recognized powerful forces shaping experience world . This recognition accompanied growing volume research , closely follows footsteps technological transformations ( e.g . radio , movies , television , internet , mobiles ) zeitgeist ( e.g . cold war , 9/11 , climate change ) attempt map mass media major impacts perceive , individuals citizens . Are media ( broadcast digital ) still able convey sense unity reaching large audiences , messages lost noisy crowd mass self-communication ?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stopwords in nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text=\"\"\"the outbreak of coronavirus disease 2019 (COVID-19) has \n",
    "created a global health crisis that has had a deep impact on the \n",
    "way we perceive our world and our everyday lives. Not only the \n",
    "rate of contagion and patterns of transmission threatens our \n",
    "sense of agency, but the safety measures put in place to contain \n",
    "the spread of the virus also require social distancing by \n",
    "refraining from doing what is inherently human, which is to find \n",
    "solace in the company of others. Within this context of physical \n",
    "threat, social and physical distancing, as well as public alarm, \n",
    "what has been (and can be) the role of the different mass media \n",
    "channels in our lives on individual, social and societal levels? \n",
    "Mass media have long been recognized as powerful forces shaping \n",
    "how we experience the world and ourselves. This recognition is \n",
    "accompanied by a growing volume of research, that closely follows \n",
    "the footsteps of technological transformations (e.g. radio, \n",
    "movies, television, the internet, mobiles) and the zeitgeist \n",
    "(e.g. cold war, 9/11, climate change) in an attempt to map mass \n",
    "media major impacts on how we perceive ourselves, both as \n",
    "individuals and citizens. Are media (broadcast and digital) still \n",
    "able to convey a sense of unity reaching large audiences, or are \n",
    "messages lost in the noisy crowd of mass self-communication? \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "my_stopwords=set(stopwords.words('english'))\n",
    "new_tokens=[]\n",
    "\n",
    "# Tokenization using word_tokenize()\n",
    "all_tokens=nltk.word_tokenize(text)\n",
    "\n",
    "for token in all_tokens:\n",
    "    if token not in my_stopwords:\n",
    "        new_tokens.append(token)\n",
    "\n",
    "\n",
    "\" \".join(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3d5ce9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccaad251",
   "metadata": {},
   "source": [
    "# 4. How to remove punctuations ?\n",
    "text=\"The match has concluded !!! India has won the match . “"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99f637d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The match has concluded India has won the match'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing punctuation in nltk with RegexpTokenizer\n",
    "\n",
    "text=\"The match has concluded !!! India has won the match . \"\n",
    "\n",
    "tokenizer=nltk.RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "tokens=tokenizer.tokenize(text)\n",
    "\" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fccae88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad7ff619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The match has concluded  India has won the match"
     ]
    }
   ],
   "source": [
    "import string\n",
    "#4. How to remove punctuations ?\n",
    "text=\"The match has concluded !!! India has won the match\"\n",
    "string.punctuation\n",
    "for i in text:\n",
    "    if i not in string.punctuation :\n",
    "        print(i,end =\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4afa6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17c1f648",
   "metadata": {},
   "source": [
    "# 5. How to perform stemming?\n",
    "text= \"\"\"Dancing is an art. Students should be taught dance as a\n",
    "subject in schools . I danced in many of my school function. Some\n",
    "people are always hesitating to dance.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781d9a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b06bcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'danc is an art . student should be taught danc as a subject in school . i danc in mani of my school function . some peopl are alway hesit to danc .'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming with nltk's PorterStemmer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text= \"\"\"Dancing is an art. Students should be taught dance as a\n",
    "subject in schools . I danced in many of my school function. Some\n",
    "people are always hesitating to dance.\"\"\"\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "stemmed_tokens=[]\n",
    "for token in nltk.word_tokenize(text):\n",
    "    stemmed_tokens.append(stemmer.stem(token))\n",
    "\n",
    "\" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c8cab2",
   "metadata": {},
   "source": [
    "# 6. How to extract usernames from emails?\n",
    " text= \"The new registrations are potter709@gmail.com , \n",
    "elixir101@gmail.com. If you find any disruptions, kindly contact \n",
    "granger111@gamil.com or severus77@gamil.com \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a8bffdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['potter709', 'elixir101', 'granger111', 'severus77']\n"
     ]
    }
   ],
   "source": [
    "# Using regular expression to extract usernames\n",
    "import re  \n",
    "\n",
    "text= \"\"\"The new registrations are potter709@gmail.com , \n",
    "elixir101@gmail.com. If you find any disruptions, kindly contact \n",
    "granger111@gamil.com or severus77@gamil.com \"\"\"\n",
    "\n",
    "# \\S matches any non-whitespace character \n",
    "# @ for as in the Email \n",
    "# + for Repeats a character one or more times \n",
    "usernames= re.findall('(\\S+)@', text)     \n",
    "print(usernames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c131d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7e620f9",
   "metadata": {},
   "source": [
    "# 7.How to find the most common words in the text excluding stopwords ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1b8a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee1d2b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junkf - F h  n g  ur b. An here' n \n",
      "nee f he n ur b bu ll we wllngl e he becue \n",
      "he re gre n e n e  ck r re  e. Junk \n",
      "f hve n r ver le nurnl vlue n rrepecve f \n",
      "he w he re rkee, he re n helh  cnue.The \n",
      "nl ren f her gnng ppulr n ncree ren f \n",
      "cnupn  h he re re  e r e  ck f. \n",
      "Peple, f ll ge grup re vng wr Junkf    \n",
      "hle free n fen re  grb n e. Cl rnk, chp, \n",
      "nle, pzz, burger, French fre ec. re few exple fr \n",
      "he gre vre f junk f vlble n he rke. Junkf \n",
      " he  ngeru f ever bu   pleure n eng n \n",
      " gve  gre e n uh exple f Junkf re kurkure \n",
      "n chp.. cl rng re l urce f junk f... he hu \n",
      "n be e n hgh un   reul fl  ur b...  \n",
      "cn be ee n  le exen ... n reerch  fun h h \n",
      "junk f r ver ngeru fr ur helh Junkf  ver \n",
      "hrful h  lwl eng w he helh f he preen \n",
      "genern. The er elf ene hw ngeru   fr ur \n",
      "be. M prnl,  e  g h peple cnue \n",
      " n  l b. Hwever, n uch wrene  pre bu \n",
      "he hrful effec f Junkf ."
     ]
    }
   ],
   "source": [
    "text=\"\"\"Junkfood - Food that do no good to our body. And there's no \n",
    "need of them in our body but still we willingly eat them because \n",
    "they are great in taste and easy to cook or ready to eat. Junk \n",
    "foods have no or very less nutritional value and irrespective of \n",
    "the way they are marketed, they are not healthy to consume.The \n",
    "only reason of their gaining popularity and increased trend of \n",
    "consumption is that they are ready to eat or easy to cook foods. \n",
    "People, of all age groups are moving towards Junkfood as it is \n",
    "hassle free and often ready to grab and eat. Cold drinks, chips, \n",
    "noodles, pizza, burgers, French fries etc. are few examples from \n",
    "the great variety of junk food available in the market. Junkfood \n",
    "is the most dangerous food ever but it is pleasure in eating and \n",
    "it gives a great taste in mouth examples of Junkfood are kurkure \n",
    "and chips.. cold rings are also source of junk food... they shud \n",
    "nt be ate in high amounts as it results fatal to our body... it \n",
    "cn be eated in a limited extend ... in research its found tht ths \n",
    "junk foods r very dangerous fr our health Junkfood is very \n",
    "harmful that is slowly eating away the health of the present \n",
    "generation. The term itself denotes how dangerous it is for our \n",
    "bodies. Most importantly, it tastes so good that people consume \n",
    "it on a daily basis. However, not much awareness is spread about \n",
    "the harmful effects of Junkfood .\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "for i in text:\n",
    "    if i not in stopwords.words('english') :\n",
    "        print(i,end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d4c40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "from nltk.corpus import brown\n",
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e5fcaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3665afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a3673bc",
   "metadata": {},
   "source": [
    "# 8.How to do spell correction in a given text?\n",
    "text=\"He is a gret person. He beleives in bod\"\n",
    "Desired Output:\n",
    "text=\"He is a great person. He believes in god\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b60216f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a great person. He believes in god\n"
     ]
    }
   ],
   "source": [
    "# Import textblob\n",
    "from textblob import TextBlob\n",
    "text=\"He is a gret person. He beleives in bod\"\n",
    "# Using textblob's correct() function\n",
    "text=TextBlob(text)\n",
    "print(text.correct())\n",
    "#> He is a great person. He believes in god"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0cee77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d01e8490",
   "metadata": {},
   "source": [
    "# 9.How to extract all the nouns in a text?\n",
    "text=\"James works at Microsoft. She lives in manchester and likes \n",
    "to play the flute\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cd6aa7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5be77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a63e018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['James', 'works', 'at', 'Microsoft', 'She', 'lives', 'in', 'manchester', 'and', 'likes', 'to', 'play', 'the', 'flute'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text=\"\"\"James works at Microsoft. She lives in manchester and likes \n",
    "to play the flute\"\"\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "blob.words # Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5200415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['james', 'microsoft'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases # Noun phrase extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac88653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035accd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79ae15c6",
   "metadata": {},
   "source": [
    "# 10.How to find the cosine similarity of two documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d106d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.45584231]\n",
      " [0.45584231 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Using Vectorizer of sklearn to get vector representation\n",
    "\n",
    "text1='Taj Mahal is a tourist place in India'\n",
    "text2='Great Wall of China is a tourist place in china'\n",
    "\n",
    "\n",
    "documents=[text1,text2]\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "vectorizer=CountVectorizer()\n",
    "matrix=vectorizer.fit_transform(documents)\n",
    "\n",
    "# Obtaining the document-word matrix\n",
    "doc_term_matrix=matrix.todense()\n",
    "doc_term_matrix\n",
    "\n",
    "# Computing cosine similarity\n",
    "df=pd.DataFrame(doc_term_matrix)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df,df))\n",
    "\n",
    "#> [[1.         0.45584231]\n",
    "#> [0.45584231 1.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585a0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
